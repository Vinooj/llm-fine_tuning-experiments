{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinooj/llm-fine_tuning-experiments/blob/main/ascii_art_completion_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXyY0QcUYXnt"
      },
      "source": [
        "## Completion finetuning using unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCuDuj793iJN"
      },
      "source": [
        "This notebook makes use of unsloth to finetune a model for a completion task.\n",
        "In this example we will finetune the llama 3.2 base model to generate ascii art. I would recommend using the unsloth library compared to just using the huggingface library as it requires less memory and is faster.\n",
        "\n",
        "Adapted from unsloth notebooks, if something is broken check on:\n",
        "https://unsloth.ai/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Automatically select the appropriate PyTorch index at runtime by inspecting the installed CUDA driver version via --torch-backend=auto\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install vllm torch torchvision torchaudio --torch-backend=auto\n",
        "\n",
        "# Install core packages without dependencies (to avoid version conflicts)\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl\n",
        "\n",
        "# Install specific triton version without dependencies\n",
        "!pip install triton==2.1.0 --no-deps\n",
        "\n",
        "# Install unsloth-related packages\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# Install remaining packages with dependencies (these are generally stable)\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer"
      ],
      "metadata": {
        "id": "8IxolDv6V-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import unsloth\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Triton: {triton.__version__}\")\n",
        "print(\"All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "NND3E8taV87K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN3IUgafYfEu"
      },
      "source": [
        "### Load base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKfnUNmFz7ws"
      },
      "outputs": [],
      "source": [
        "# Import FastLanguageModel instead of AutoModelForCausalLM.from_pretrained form\n",
        "# Huggingface which leverages Optimized Kernels, Efficient Memory Management,\n",
        "# Smart Data Type Handling ( Precision) to improve training speed\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "\n",
        "# ets the maximum number of tokens that this specific instance of the model and\n",
        "# its tokenizer will be configured to handle during our finetuning and subsequent\n",
        "# inference.\n",
        "max_seq_length = 2048\n",
        "\n",
        "\n",
        "# we are telling Unsloth to automatically determine the most suitable data type\n",
        "#(precision) for the model based on the available hardware (like your GPU).\n",
        "# Unsloth is designed to leverage faster and more memory-efficient data types,\n",
        "#such as bfloat16 or float16, if your hardware supports them.\n",
        "dtype = None\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"meta-llama/Llama-3.2-3B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit = False,\n",
        "    token=userdata.get('HF_TOKEN')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-eg28bp9CKq"
      },
      "outputs": [],
      "source": [
        "tokenizer.clean_up_tokenization_spaces = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRARyu6GYiK1"
      },
      "source": [
        "### Add lora to base model and patch with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYobA8h40U9E"
      },
      "outputs": [],
      "source": [
        "# More info about parameters: https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraConfig\n",
        "target_modules =  [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# When adding special tokens\n",
        "train_embeddings = False\n",
        "\n",
        "if train_embeddings:\n",
        "  target_modules = target_modules + [\"lm_head\"]\n",
        "\n",
        "# PEFT stands for \"Parameter-Efficient Finetuning,\" and Unsloth integrates with PEFT methods like LoRA, QLoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,              # A rank of 16 is a common value that balances expressiveness with\n",
        "                         # parameter efficiency. A higher rank means more parameters in the\n",
        "                         # LoRA adapters, allowing for more complex changes but also increasing the risk of overfitting slightly\n",
        "    target_modules = target_modules,  # On which modules of the llm the lora weights are used\n",
        "    lora_alpha = 16,     # scales the weights of the adapters (more influence on base model), 16 was recommended on reddit\n",
        "                         # Having a value same as r, lora_alpha/r = 1 is the normal.\n",
        "    lora_dropout = 0,    # Default on 0.05 in tutorial but unsloth says 0 is better, This is a regularization technique\n",
        "    bias = \"none\",       # \"none\" is optimized. Contributes to VRAM (GPU memory) and improving training efficiency.\n",
        "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\" for very long context, decreases vram. Contributes to VRAM (GPU memory) and improving training efficiency.\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # scales lora_alpha with 1/sqrt(r), huggingface says this works better.\n",
        "                         # Now, let's look at use_rslora = False. This parameter controls whether\n",
        "                         # \"Rank-Stabilized LoRA\" is used. Rank-Stabilized LoRA is a variation\n",
        "                         # where the LoRA adapter's output is scaled by lora_alpha / sqrt(r) instead of lora_alpha / r\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYsJICcT07cN"
      },
      "outputs": [],
      "source": [
        "empty_prompt = \"\"\"\n",
        "{ascii_art}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func_no_prompt(examples):\n",
        "  ascii_art_samples = examples[\"ascii\"]\n",
        "  training_prompts = []\n",
        "  for ascii_art in ascii_art_samples:\n",
        "      training_prompt = empty_prompt.format(ascii_art=ascii_art) + EOS_TOKEN\n",
        "      training_prompts.append(training_prompt)\n",
        "  return { \"text\" : training_prompts, }\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"pookie3000/ascii-cats\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func_no_prompt, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghijf8GxNORq"
      },
      "source": [
        " ### Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeQ6cV5BMn6-"
      },
      "outputs": [],
      "source": [
        "for i, sample in enumerate(dataset):\n",
        "    print(f\"\\n------ Sample {i + 1} ----\")\n",
        "    print(sample[\"text\"])\n",
        "    if i > 2:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PNMTxya08bi"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "# from transformers import TrainingArguments\n",
        "# from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzHaVYIP1AKp"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UlSmY8m4grb"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IqA3IbOcTuz"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "def generate_ascii_art(model):\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    inputs = tokenizer(\"\", return_tensors = \"pt\").to(\"cuda\")\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationMixin\n",
        "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "    for token in model.generate(**inputs, streamer = text_streamer, max_new_tokens = 100):\n",
        "        print(token)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLxH5W-2cmrH"
      },
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "  generate_ascii_art(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKQLPkQGP3hR"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOmWufx3iJS"
      },
      "source": [
        "### Save lora adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi0YGlA03iJS"
      },
      "source": [
        "This is both useful for inference and if you want to load the model again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JJpQyOS3iJS"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\n",
        "    \"vinooj/Llama-3.2-3B-ascii-cats-lora\",\n",
        "    tokenizer,\n",
        "    token = userdata.get('HF_WRITE_TOKEN')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W1hcfmv3iJS"
      },
      "source": [
        "### Merge model with lora weights and save to gguf\n",
        "\n",
        "You can then do inference locally with Ollama or llama.cpp\n",
        "\n",
        "##### Popular quantization methods\n",
        "\n",
        "- **q4_k_m**  \n",
        "  4bit quantization. Low memory. All models you pull with ollama uses this quantization.\n",
        "- **q8_0**  \n",
        "  8bit quantization. Medium memory.\n",
        "- **f16**  \n",
        "  16 bit quantization. A lot of models are already in 16 bit so then no quantization happens\n",
        "- **not_quantized**  \n",
        "  Often same as f16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwSp6tH51FoG"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\n",
        "    \"vinooj/Llama-3.2-3B-ascii-cats-lora-q4_k_m-GGUF\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    token = userdata.get('HF_WRITE_TOKEN')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "YTSkhEZQ_R8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fPc6YWq3iJS"
      },
      "source": [
        "### Load model and saved lora adapters\n",
        "For if you want to continue finetuning or want to do inference using the model in safetensor format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eK5Xy4J3iJS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"vinooj/Llama-3.2-3B-ascii-cats-lora\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        "    token=userdata.get('HF_TOKEN')\n",
        ")\n",
        "\n",
        "\n",
        "def generate_ascii_art(model):\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    inputs = tokenizer(\"\", return_tensors = \"pt\").to(\"cuda\")\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationMixin\n",
        "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "    for token in model.generate(**inputs, streamer = text_streamer, max_new_tokens = 100):\n",
        "        print(token)\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_ascii_art(model)"
      ],
      "metadata": {
        "id": "JmIxxZIpAfV5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}