{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinooj/llm-fine_tuning-experiments/blob/main/fine_tuning_for_tool_support_with_grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXyY0QcUYXnt"
      },
      "source": [
        "## Using GRPO (Group Relative Policy Optimization) for tool-calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCuDuj793iJN"
      },
      "source": [
        "Here is a complete, self-contained Python script to fine-tune the Phi-3-base model for general-purpose tool calling using GRPO.\n",
        "\n",
        "This code includes a minimal dataset, the dynamic prompt structure, and a robust, general-purpose reward function that validates tool calls against a provided schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 1: Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IxolDv6V-Ij"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Automatically select the appropriate PyTorch index at runtime by inspecting the installed CUDA driver version via --torch-backend=auto\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install vllm torch torchvision torchaudio --torch-backend=auto\n",
        "\n",
        "# Install core packages without dependencies (to avoid version conflicts)\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl\n",
        "\n",
        "# Install specific triton version without dependencies\n",
        "!pip install triton==2.1.0 --no-deps\n",
        "\n",
        "# Install unsloth-related packages\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# Install remaining packages with dependencies (these are generally stable)\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NND3E8taV87K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import unsloth\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Triton: {triton.__version__}\")\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN3IUgafYfEu"
      },
      "source": [
        "### Load base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKfnUNmFz7ws"
      },
      "outputs": [],
      "source": [
        "# Import FastLanguageModel instead of AutoModelForCausalLM.from_pretrained form\n",
        "# Huggingface which leverages Optimized Kernels, Efficient Memory Management,\n",
        "# Smart Data Type Handling ( Precision) to improve training speed\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from google.colab import userdata\n",
        "from unsloth import FastLanguageModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRARyu6GYiK1"
      },
      "source": [
        "### Cell 2: Imports and Model Loading with Unsloth\n",
        "This is the key change. We use FastLanguageModel from Unsloth, which automatically handles the backend optimizations for significantly faster training and lower memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYobA8h40U9E"
      },
      "outputs": [],
      "source": [
        "# More info about parameters: https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraConfig\n",
        "target_modules =  [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# When adding special tokens\n",
        "train_embeddings = False\n",
        "\n",
        "if train_embeddings:\n",
        "  target_modules = target_modules + [\"lm_head\"]\n",
        "\n",
        "# Sets the maximum number of tokens that this specific instance of the model and\n",
        "# its tokenizer will be configured to handle during our finetuning and subsequent\n",
        "# inference.\n",
        "max_seq_length = 2048\n",
        "\n",
        "\n",
        "# we are telling Unsloth to automatically determine the most suitable data type\n",
        "#(precision) for the model based on the available hardware (like your GPU).\n",
        "# Unsloth is designed to leverage faster and more memory-efficient data types,\n",
        "#such as bfloat16 or float16, if your hardware supports them.\n",
        "dtype = None\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/phi-3-mini-4k-instruct\", # Use the Unsloth version for optimizations\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit = True,\n",
        "    token=userdata.get('HF_TOKEN')\n",
        ")\n",
        "\n",
        "# tokenizer.clean_up_tokenization_spaces = False 'Came from the old ascii_art code'\n",
        "\n",
        "# PEFT stands for \"Parameter-Efficient Finetuning,\" and Unsloth integrates with PEFT methods like LoRA, QLoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,              # A rank of 16 is a common value that balances expressiveness with\n",
        "                         # parameter efficiency. A higher rank means more parameters in the\n",
        "                         # LoRA adapters, allowing for more complex changes but also increasing the risk of overfitting slightly\n",
        "    target_modules = target_modules,  # On which modules of the llm the lora weights are used\n",
        "    lora_alpha = 16,     # scales the weights of the adapters (more influence on base model), 16 was recommended on reddit\n",
        "                         # Having a value same as r, lora_alpha/r = 1 is the normal.\n",
        "    lora_dropout = 0,    # Default on 0.05 in tutorial but unsloth says 0 is better, This is a regularization technique\n",
        "    bias = \"none\",       # \"none\" is optimized. Contributes to VRAM (GPU memory) and improving training efficiency.\n",
        "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\" for very long context, decreases vram. Contributes to VRAM (GPU memory) and improving training efficiency.\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # scales lora_alpha with 1/sqrt(r), huggingface says this works better.\n",
        "                         # Now, let's look at use_rslora = False. This parameter controls whether\n",
        "                         # \"Rank-Stabilized LoRA\" is used. Rank-Stabilized LoRA is a variation\n",
        "                         # where the LoRA adapter's output is scaled by lora_alpha / sqrt(r) instead of lora_alpha / r\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "# The Phi-3 Instruct model already has a chat template, so we don't need to set it manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 3: The Training Dataset\n",
        "This section defines our small, diverse dataset for teaching tool use. This remains unchanged from the original script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYsJICcT07cN"
      },
      "outputs": [],
      "source": [
        "# @title 3. The Training Dataset\n",
        "# A minimal, diverse dataset to teach tool use and when NOT to use tools.\n",
        "training_data = [\n",
        "    {\n",
        "        \"prompt\": \"What is the weather like in New York City?\",\n",
        "        \"is_negative\": False,\n",
        "        \"tools\": [\n",
        "            {\n",
        "                \"name\": \"get_weather\",\n",
        "                \"description\": \"Fetches the current weather for a given city.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"city\": {\"type\": \"string\", \"description\": \"The city name.\"}\n",
        "                    },\n",
        "                    \"required\": [\"city\"],\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Please send an email to john.doe@example.com with the subject 'Hello' and body 'How are you?'\",\n",
        "        \"is_negative\": False,\n",
        "        \"tools\": [\n",
        "            {\n",
        "                \"name\": \"send_email\",\n",
        "                \"description\": \"Sends an email.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"recipient\": {\"type\": \"string\"},\n",
        "                        \"subject\": {\"type\": \"string\"},\n",
        "                        \"body\": {\"type\": \"string\"},\n",
        "                    },\n",
        "                    \"required\": [\"recipient\", \"subject\", \"body\"],\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Hello, how are you today?\",\n",
        "        \"is_negative\": True, # This is a negative sample. No tool should be called.\n",
        "        \"tools\": [\n",
        "            {\"name\": \"get_weather\", \"description\": \"Gets the weather.\"},\n",
        "            {\"name\": \"send_email\", \"description\": \"Sends an email.\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"What's the current price of the AAPL stock?\",\n",
        "        \"is_negative\": False,\n",
        "        \"tools\": [\n",
        "            {\"name\": \"search_news\", \"description\": \"Searches for news articles.\"},\n",
        "            {\n",
        "                \"name\": \"get_stock_price\",\n",
        "                \"description\": \"Gets the current price of a stock symbol.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\"symbol\": {\"type\": \"string\"}},\n",
        "                    \"required\": [\"symbol\"],\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "# Convert to Hugging Face Dataset object\n",
        "dataset = Dataset.from_list(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 4: Dynamic Prompt Formatting\n",
        "This function dynamically creates the system prompt with the available tools for each specific training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PNMTxya08bi"
      },
      "outputs": [],
      "source": [
        "# @title 4. Dynamic Prompt Formatting\n",
        "# This function creates the full prompt the model will see.\n",
        "def create_prompt(sample):\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. You have access to the following tools. \"\n",
        "        \"When a user's request can be fulfilled by a tool, respond with a tool call in the format: \"\n",
        "        \"<tool_call>{\\\"name\\\": \\\"tool_name\\\", \\\"arguments\\\": {\\\"arg1\\\": \\\"value1\\\"}}</tool_call>. \"\n",
        "        \"If no tool is appropriate, answer conversationally.\"\n",
        "    )\n",
        "    # Add the JSON representation of the tools to the system message\n",
        "    system_message += \"\\nAvailable Tools:\\n\" + json.dumps(sample[\"tools\"], indent=2)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": sample[\"prompt\"]},\n",
        "    ]\n",
        "\n",
        "    # We only need the formatted prompt for the trainer\n",
        "    prompt_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    sample[\"formatted_prompt\"] = prompt_str\n",
        "    return sample\n",
        "\n",
        "# Apply the formatting to the entire dataset\n",
        "dataset = dataset.map(create_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 5: The General-Purpose Reward Function\n",
        "This is the core of the GRPO logic. The function scores the model's generated tool calls based on their validity and adherence to the provided schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IqA3IbOcTuz"
      },
      "outputs": [],
      "source": [
        "# @title 5. The General-Purpose Reward Function\n",
        "def reward_function(generated_responses: list[str], sample: dict) -> torch.Tensor:\n",
        "    rewards = []\n",
        "    tool_schemas = {tool['name']: tool for tool in sample['tools']}\n",
        "    is_negative_sample = sample.get('is_negative', False)\n",
        "\n",
        "    for response in generated_responses:\n",
        "        score = 0.0\n",
        "        # Regex to find the tool call within the response\n",
        "        match = re.search(r\"<tool_call>(.*?)</tool_call>\", response, re.DOTALL)\n",
        "\n",
        "        if match:\n",
        "            # If a tool call is found\n",
        "            if is_negative_sample:\n",
        "                # Penalty for calling a tool on a negative sample\n",
        "                score -= 5.0\n",
        "            else:\n",
        "                try:\n",
        "                    tool_call_str = match.group(1)\n",
        "                    tool_call_obj = json.loads(tool_call_str)\n",
        "                    tool_name = tool_call_obj.get(\"name\")\n",
        "                    arguments = tool_call_obj.get(\"arguments\", {})\n",
        "\n",
        "                    if tool_name in tool_schemas:\n",
        "                        score += 2.0  # Reward for selecting a valid tool\n",
        "                        schema = tool_schemas[tool_name].get(\"parameters\", {})\n",
        "                        required_args = schema.get(\"required\", [])\n",
        "\n",
        "                        # Reward for providing all required arguments\n",
        "                        for req_arg in required_args:\n",
        "                            if req_arg in arguments:\n",
        "                                score += 1.0\n",
        "                            else:\n",
        "                                score -= 2.0 # Heavy penalty for missing required arg\n",
        "\n",
        "                        # Penalize extraneous arguments\n",
        "                        for arg in arguments:\n",
        "                            if arg not in schema.get(\"properties\", {}):\n",
        "                                score -= 1.0\n",
        "                    else:\n",
        "                        score -= 3.0 # Penalty for calling a non-existent tool\n",
        "                except json.JSONDecodeError:\n",
        "                    score -= 5.0 # Heavy penalty for malformed JSON\n",
        "        else:\n",
        "            # If no tool call is found\n",
        "            if is_negative_sample:\n",
        "                score += 5.0 # High reward for correctly ignoring a negative sample\n",
        "            else:\n",
        "                # Penalty for not calling a tool when one was likely needed\n",
        "                score -= 2.0\n",
        "\n",
        "        rewards.append(score)\n",
        "\n",
        "    return torch.tensor(rewards, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 6: GRPO Trainer Setup and Execution\n",
        "Here, we configure and launch the GRPOTrainer. Thanks to Unsloth, this process will be 2-3 times faster than the standard implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 6. GRPO Trainer Setup and Execution\n",
        "# GRPO Configuration\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=\"./grpo_phi3_tool_caller\",\n",
        "    num_train_epochs=5, # Increased epochs for better learning on a small dataset\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    max_prompt_length=2048, # Corresponds to max_seq_length\n",
        "    max_completion_length=512,\n",
        "    beta=0.1,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_8bit\", # Use 8-bit AdamW optimizer\n",
        ")\n",
        "\n",
        "# Initialize the GRPO Trainer\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=grpo_config,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    reward_function=reward_function,\n",
        "    prompt_col=\"formatted_prompt\",\n",
        ")\n",
        "\n",
        "# Start training!\n",
        "print(\"Starting GRPO training with Unsloth...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Save the trained LoRA adapter\n",
        "trainer.save_model(\"./grpo_phi3_tool_caller/final_adapter\")\n",
        "print(\"Model adapter saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 7: Inference\n",
        "Finally, let's test our newly trained model with a complex prompt that requires using multiple tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 7. Inference\n",
        "# Use Unsloth's fast inference\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Use the Phi-3 chat template\n",
        ")\n",
        "\n",
        "# Create a test sample\n",
        "test_sample = {\n",
        "    \"prompt\": \"I need to know the weather in London and also send a confirmation email to supervisor@company.com with the subject 'Task Complete'\",\n",
        "    \"tools\": [\n",
        "        {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Fetches the current weather.\",\n",
        "            \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\"}}, \"required\": [\"city\"]},\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"send_email\",\n",
        "            \"description\": \"Sends an email.\",\n",
        "            \"parameters\": {\"type\": \"object\", \"properties\": {\"recipient\": {\"type\": \"string\"}, \"subject\": {\"type\": \"string\"}}, \"required\": [\"recipient\", \"subject\"]},\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Format the prompt using the same function as in training\n",
        "formatted_prompt = create_prompt(test_sample)[\"formatted_prompt\"]\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
        "response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"\\nUser Prompt:\\n\", test_sample[\"prompt\"])\n",
        "print(\"\\nModel Response:\\n\", response_text.split(\"<|assistant|>\")[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
