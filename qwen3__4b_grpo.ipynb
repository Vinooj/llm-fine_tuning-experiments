{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinooj/llm-fine_tuning-experiments/blob/main/qwen3__4b_grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTxyb8Av4vZ"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2IL9EzYv4vb"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoUa3kosv4vb"
      },
      "source": [
        "**NEW** Unsloth now supports training the new **gpt-oss** model from OpenAI! You can start finetune gpt-oss for free with our **[Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)**!\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdJCh7ENv4vb"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcUTuqa5v4vb"
      },
      "outputs": [],
      "source": [
        "# This code block checks if the environment is Google Colab and installs the necessary libraries (unsloth and vllm) using pip if not in Colab.\n",
        "# The %%capture magic command suppresses the output of the installation process.\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfIYqIPEv4vc"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.55.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkH_y8UC9lvv"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN75nmdx9lvw"
      },
      "source": [
        "Goal: To convert `Qwen3-4B-Base` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
        "\n",
        "We first pre fine-tune the model to make GRPO skip trying to match formatting - this speeds GRPO up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DuiVRLhMco"
      },
      "source": [
        "### GRPO chat template\n",
        "Since we're using a base model, we should set a chat template. You can make your own chat template as well!\n",
        "1. DeepSeek uses `<think>` and `</think>`, but this is **not** necessary - you can customize it however you like!\n",
        "2. A `system_prompt` is recommended to at least guide the model's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UjowCbT-cFz"
      },
      "outputs": [],
      "source": [
        "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
        "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
        "solution_start  = \"<SOLUTION>\"\n",
        "solution_end    = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between <start_working_out> and <end_working_out>.\n",
        "Then, provide your solution between <SOLUTION></SOLUTION>\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgs0MJkDkYL"
      },
      "source": [
        "We create a simple chat template below. Notice `add_generation_prompt` includes prepending `<start_working_out>` to guide the model to start its reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3fF9gMujY02"
      },
      "outputs": [],
      "source": [
        "chat_template = \\\n",
        "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
        "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages[1:] %}\"\\\n",
        "    \"{% else %}\"\\\n",
        "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages %}\"\\\n",
        "    \"{% endif %}\"\\\n",
        "    \"{% for message in loop_messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ message['content'] }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ message['content'] + eos_token }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "\n",
        "# Replace with out specific template:\n",
        "chat_template = chat_template\\\n",
        "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
        "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEcLdymBEHdk"
      },
      "source": [
        "Let's see how our chat template behaves on an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BciEDYSSYFNj"
      },
      "outputs": [],
      "source": [
        "tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : f\"<start_working_out>I think it's 2.<end_working_out><SOLUTION>2</SOLUTION>\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
        "], tokenize = False, add_generation_prompt = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mdsuGjxHrjT"
      },
      "source": [
        "### Pre fine-tuning for formatting\n",
        "We now use a subset of NVIDIA's [Open Math Reasoning dataset](https://huggingface.co/datasets/nvidia/OpenMathReasoning) which was filtered to only include high quality DeepSeek R1 traces.\n",
        "\n",
        "We'll only filter ~59 or so examples to first \"prime\" / pre fine-tune the model to understand our custom GRPO formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXxM2lStVIkd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
        "dataset = dataset.to_pandas()[\n",
        "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
        "]\n",
        "\n",
        "# Try converting to number - if not, replace with NaN\n",
        "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
        "# Select only numbers\n",
        "dataset = dataset.iloc[np.where(is_number)[0]]\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVRFqoSdIEVK"
      },
      "source": [
        "We have to format the dataset to follow our GRPO style formatting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ydcV_Abfi6"
      },
      "outputs": [],
      "source": [
        "def format_dataset(x):\n",
        "    expected_answer = x[\"expected_answer\"]\n",
        "    problem = x[\"problem\"]\n",
        "\n",
        "    # Remove generated <think> and </think>\n",
        "    thoughts = x[\"generated_solution\"]\n",
        "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
        "\n",
        "    # Strip newlines on left and right\n",
        "    thoughts = thoughts.strip()\n",
        "    # Add our custom formatting\n",
        "    final_prompt = \\\n",
        "        reasoning_start + thoughts + reasoning_end + \\\n",
        "        solution_start + expected_answer + solution_end\n",
        "    return [\n",
        "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
        "        {\"role\" : \"user\",      \"content\" : problem},\n",
        "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
        "    ]\n",
        "\n",
        "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5NI47rOIRP2"
      },
      "source": [
        "Check to see if it worked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTdXBKcslhRH"
      },
      "outputs": [],
      "source": [
        "tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHV9BXYiIYaq"
      },
      "source": [
        "Let's truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces.\n",
        "\n",
        "Note this might take 2 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBHFlRbae9_s"
      },
      "outputs": [],
      "source": [
        "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
        "\n",
        "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6NkUCAGIj8N"
      },
      "source": [
        "We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rgdtiV_f5hx"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n",
        "dataset = Dataset.from_pandas(dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQJjQrYKzOk"
      },
      "source": [
        "Let's now pre fine-tune the model so it follows our custom GRPO formatting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woYi0SSygpqp"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4-2v_bLhZuE"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRMBNUBgLC8T"
      },
      "source": [
        "Let's check if the model has learnt to follow the custom format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HJxrS76h3Ds"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    dataset[0][\"Messages\"][:2],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 0,\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtZ3qGOALF95"
      },
      "source": [
        "Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWSZ0DET7bob"
      },
      "outputs": [],
      "source": [
        "del dataset\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7-eUrQn-OzE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b00gUsS-ROW"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siopxjG8-ReF"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGupRQqD-Wcf"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"solution\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnXj6hn-Ydi"
      },
      "source": [
        "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JJGXKdJ-Zl_"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text):\n",
        "    # if \"####\" not in text: return None\n",
        "    # return text.split(\"####\")[1].strip()\n",
        "    return text\n",
        "extract_hash_answer(dataset[0][\"solution\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30CygaU-dir"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyEVI972-d3n"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9m8eR9T-gMh"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQwjTjNz-gY_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Add optional EOS token matching\n",
        "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
        "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OycMneOq-iNC"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndzHnQ_6-jHt"
      },
      "outputs": [],
      "source": [
        "match_format.findall(\n",
        "    \"Let me think!<end_working_out>\"\\\n",
        "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRMDAzDk2x6t"
      },
      "outputs": [],
      "source": [
        "match_format.findall(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    f\"<SOLUTION>  2  </SOLUTION>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOjmO5l-kl3"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "\n",
        "        # No need to reward <start_working_out> since we always prepend it!\n",
        "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAUWwtE-s6n"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmtI_8gg-uIE"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        # Correct answer gets 5 points!\n",
        "        if guess == true_answer:\n",
        "            score += 5.0\n",
        "        # Match if spaces are seen, but less reward\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 3.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
        "                else: score -= 2.5 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 4.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atMyfhXh-v3R"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
        "\n",
        "We also remove possible commas for example as in 123,456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVW0kL8q-wL5"
      },
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbfaaAywNHHh"
      },
      "source": [
        "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjBFrttr-y1_"
      },
      "outputs": [],
      "source": [
        "global PRINTED_TIMES\n",
        "PRINTED_TIMES = 0\n",
        "global PRINT_EVERY_STEPS\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    # Print only every few steps\n",
        "    global PRINTED_TIMES\n",
        "    global PRINT_EVERY_STEPS\n",
        "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
        "        print(\n",
        "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "    PRINTED_TIMES += 1\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(-2.5)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            # Remove commas like in 123,456\n",
        "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
        "            scores.append(3.5 if guess == true_answer else -1.5)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgOR3wJ_AyLr"
      },
      "source": [
        "Get the top 90% prompt length so we don't accidentally truncate them!\n",
        "\n",
        "Ie we'll remove the top 10% long prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EgAi4Q5fGE-"
      },
      "outputs": [],
      "source": [
        "tokenized = dataset.map(\n",
        "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
        "    batched = True,\n",
        ")\n",
        "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
        "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
        "\n",
        "import numpy as np\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(\"Max Length = \", maximum_length)\n",
        "\n",
        "# Filter only samples smaller than 90% max length\n",
        "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "del tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-6,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 100,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = \"What is the sqrt of 101?\"\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4LMOBl8boGX"
      },
      "source": [
        "Verify LoRA is actually trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfdI-ERbpiw"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
        "    # Verify both A and B are non zero\n",
        "    for key in f.keys():\n",
        "        tensor = f.get_tensor(key)\n",
        "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
        "        assert(n_zeros.item() != tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V15Yhj1V9lwG"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13fa05b"
      },
      "source": [
        "# Task\n",
        "Explain the provided notebook code step-by-step, focusing on concepts like LoRA, GRPO, reward functions, and data preparation, to help a user new to these topics understand the process of fine-tuning a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "475aec51"
      },
      "source": [
        "## Explain lora\n",
        "\n",
        "### Subtask:\n",
        "Add markdown cells explaining what LoRA is and why it's used in this context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbc63524"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to insert a markdown cell explaining LoRA before the code cell with id DkIvEkIIkEyB.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7f790f6"
      },
      "source": [
        "This code block is specifically for Google Colab and Kaggle environments. It uses `uv pip install` to install the required libraries, including `unsloth`, `vllm`, `numpy`, `torchvision`, `bitsandbytes`, and `xformers`. It also checks for the presence of a Tesla T4 GPU to install specific versions of `vllm` and `triton` for optimal performance. Finally, it installs a specific version of the `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0656ffe8"
      },
      "source": [
        "This code block initializes the base language model (`unsloth/Qwen3-4B-Base`) and its tokenizer using `FastLanguageModel.from_pretrained`.\n",
        "\n",
        "*   `max_seq_length`: Sets the maximum sequence length the model can handle.\n",
        "*   `lora_rank`: Defines the rank for the LoRA adapters, influencing the number of trainable parameters.\n",
        "*   `load_in_4bit = False`: Specifies that the model should be loaded in 16-bit precision for LoRA training (4-bit is for inference).\n",
        "*   `fast_inference = True`: Enables vLLM for faster inference.\n",
        "*   `max_lora_rank`: Sets the maximum LoRA rank allowed.\n",
        "*   `gpu_memory_utilization`: Controls the GPU memory allocation for vLLM.\n",
        "\n",
        "After loading the model, `FastLanguageModel.get_peft_model` is used to prepare the model for PEFT (Parameter-Efficient Fine-Tuning) with LoRA.\n",
        "\n",
        "*   `r`: Sets the LoRA rank.\n",
        "*   `target_modules`: Specifies which layers of the model will have LoRA adapters injected.\n",
        "*   `lora_alpha`: A scaling factor for the LoRA updates.\n",
        "*   `use_gradient_checkpointing = \"unsloth\"`: Enables gradient checkpointing to reduce memory usage during training.\n",
        "*   `random_state`: Sets a random seed for reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8f055da"
      },
      "source": [
        "This code block defines the custom tokens and the system prompt that will be used to structure the input and output during the GRPO training.\n",
        "\n",
        "*   `reasoning_start`, `reasoning_end`: Tokens to mark the beginning and end of the model's step-by-step reasoning process.\n",
        "*   `solution_start`, `solution_end`: Tokens to mark the beginning and end of the final solution.\n",
        "*   `system_prompt`: A guiding instruction for the model, explaining its role and how to format its output using the defined tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c859950"
      },
      "source": [
        "This code block defines a custom chat template using the Jinja templating language. This template structures the conversation turns between the system, user, and assistant roles, ensuring that the model receives input and generates output in a consistent format that aligns with the GRPO training objective. It also prepends the `reasoning_start` token when generating a new response to encourage the model to begin with its thought process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eca1503"
      },
      "source": [
        "This code block demonstrates how the custom chat template defined in the previous cell formats a sample conversation. It applies the template to a list of messages with different roles and shows the resulting string that will be fed into the model. The `add_generation_prompt = True` argument ensures that the `reasoning_start` token is added at the end of the prompt for the model to start generating its response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31b05730"
      },
      "source": [
        "This code block loads a subset of the NVIDIA Open Math Reasoning dataset from the Hugging Face Hub using `load_dataset`. It then converts the dataset to a pandas DataFrame and filters it to include only examples where the `expected_answer` can be converted to a number. This filtered dataset will be used for the initial pre-fine-tuning step to teach the model the desired output format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0565a2a8"
      },
      "source": [
        "This code block defines a function `format_dataset` that takes a row from the dataset and formats it into the desired GRPO chat template structure. It extracts the problem, expected answer, and generated solution (reasoning), cleans up the reasoning by removing existing `<think>` tokens, and then constructs the final prompt and assistant response using the custom `reasoning_start`, `reasoning_end`, `solution_start`, and `solution_end` tokens. The function then applies this formatting to the entire dataset, creating a new \"Messages\" column containing the formatted conversations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749d87fa"
      },
      "source": [
        "This code block checks the output of the `format_dataset` function by applying the tokenizer's chat template to the first example in the \"Messages\" column of the dataset. This helps verify that the dataset has been formatted correctly according to the custom chat template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876b3c4b"
      },
      "source": [
        "This code block calculates the number of tokens for each formatted message in the dataset and stores it in a new column \"N\". It then filters the dataset to keep only those examples where the number of tokens is less than or equal to half of the `max_seq_length`. This truncation step is performed to prevent excessively long reasoning traces during the pre-fine-tuning, which helps manage memory usage and training efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3a311e4"
      },
      "source": [
        "This code block prepares the dataset for training with the `trl` library's `SFTTrainer`. It applies the chat template to the \"Messages\" column to create a \"text\" column containing the fully formatted prompt and response as a single string. Then, it converts the pandas DataFrame back into a Hugging Face `Dataset` object, which is the required format for the `SFTTrainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b57881f"
      },
      "source": [
        "This code block sets up the `SFTTrainer` for the pre-fine-tuning phase.\n",
        "\n",
        "*   `model`, `tokenizer`: The LoRA-enabled model and its tokenizer.\n",
        "*   `train_dataset`: The formatted dataset prepared in the previous steps.\n",
        "*   `args`: An `SFTConfig` object containing various training parameters:\n",
        "    *   `dataset_text_field`: Specifies the column in the dataset that contains the training text.\n",
        "    *   `per_device_train_batch_size`, `gradient_accumulation_steps`: Control the effective batch size for training.\n",
        "    *   `warmup_steps`: Number of steps for a learning rate warmup.\n",
        "    *   `num_train_epochs`: Number of full passes over the training data.\n",
        "    *   `learning_rate`: The learning rate for the optimizer.\n",
        "    *   `logging_steps`: How often to log training progress.\n",
        "    *   `optim`: The optimizer to use.\n",
        "    *   `weight_decay`: L2 regularization parameter.\n",
        "    *   `lr_scheduler_type`: Type of learning rate scheduler.\n",
        "    *   `seed`: Random seed for reproducibility.\n",
        "    *   `report_to`: Where to report training metrics (e.g., \"none\" to disable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f43be2bd"
      },
      "source": [
        "This code block starts the training process for the pre-fine-tuning phase using the `trainer.train()` method. This step trains the LoRA adapters on the formatted dataset to teach the model the desired output structure and the use of the custom GRPO tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3a23d3"
      },
      "source": [
        "This code block tests if the model has learned to follow the custom GRPO format after the pre-fine-tuning. It constructs a prompt using the chat template and then uses `model.generate` to produce a response. The `TextStreamer` is used to display the generated text as it's being produced. The goal is to see if the model starts its response with the `reasoning_start` token and attempts to follow the defined structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637f2d2b"
      },
      "source": [
        "This code block cleans up memory after the pre-fine-tuning step. It deletes the `dataset` variable and uses `torch.cuda.empty_cache()` and `gc.collect()` to free up GPU memory and clear the garbage collector, which is important before moving to the next training phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3b4538"
      },
      "source": [
        "This code block loads the main dataset for GRPO training, which is the \"open-r1/DAPO-Math-17k-Processed\" dataset from the Hugging Face Hub. It specifies the \"en\" configuration and loads the \"train\" split. This dataset contains a larger collection of math problems and solutions that will be used for the main GRPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30e45150"
      },
      "source": [
        "This code block displays the \"prompt\" field of the first example in the loaded dataset. This shows the input question for a math problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a56264d8"
      },
      "source": [
        "This code block displays the \"solution\" field of the first example in the loaded dataset. This shows the expected solution, which includes both the reasoning steps and the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3905c2"
      },
      "source": [
        "This code block defines a function `extract_hash_answer`. Although commented out, the original intention was likely to extract the final answer from solutions that used a \"####\" marker, as is common in datasets like GSM8K. However, for the Open R1 dataset used here, the function simply returns the entire solution string as the \"answer\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfec8223"
      },
      "source": [
        "This code block maps the dataset to apply the formatting for the GRPO training. It creates a \"prompt\" field containing the system prompt and user question in the chat template format. It also creates an \"answer\" field by applying the `extract_hash_answer` function to the \"solution\" field. Finally, it displays the first example of the processed dataset to show the new structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e819a14"
      },
      "source": [
        "This code block defines a regular expression pattern `match_format` to extract the final answer from the model's generated output based on the custom GRPO tokens.\n",
        "\n",
        "*   `rf\"{reasoning_end}.*?\"`: Matches the `reasoning_end` token followed by any characters (non-greedily).\n",
        "*   `rf\"{solution_start}(.+?){solution_end_regex}\"`: Matches the `solution_start` token, captures any characters in between (the solution) non-greedily using `(.+?)`, and then matches the `solution_end_regex`.\n",
        "*   `solution_end_regex`: This part is defined separately to optionally match the end-of-sequence token (`eos_token`) after the `solution_end` token.\n",
        "*   `[\\s]{{0,}}$`: Matches zero or more whitespace characters at the end of the string.\n",
        "*   `flags = re.MULTILINE | re.DOTALL`: Enables multiline matching and allows the dot (`.`) to match newline characters.\n",
        "\n",
        "This regex is crucial for extracting the model's predicted answer from its structured output during GRPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ebbe63d"
      },
      "source": [
        "This code block verifies that the `match_format` regular expression works correctly by applying it to a sample string that follows the expected GRPO output format. It uses `findall` to extract the captured group, which should be the content between the `solution_start` and `solution_end` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5fe301"
      },
      "source": [
        "This code block further verifies the `match_format` regular expression with another sample string, including leading/trailing whitespace around the solution and newline characters. This ensures the regex is robust to variations in whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1a31ff"
      },
      "source": [
        "This code block defines the `match_format_exactly` reward function. This function is used in GRPO to reward the model based on whether its generated response exactly matches the expected format defined by the custom tokens and their order. If the `match_format` regex successfully finds a match in the completion, the model receives a score of 3.0; otherwise, it receives 0. This encourages the model to adhere strictly to the desired output structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3e6aab"
      },
      "source": [
        "This code block defines the `match_format_approximately` reward function. This function provides a partial reward if the model's output includes some of the required formatting tokens, even if it doesn't match the exact structure. It checks for the presence of `reasoning_end`, `solution_start`, and `solution_end` tokens. The model gets a positive score (0.5) for each token present exactly once and a negative score (-1.0) otherwise. This helps guide the model towards the correct format even when it doesn't get it perfectly right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624d8695"
      },
      "source": [
        "This code block defines the `check_answer` reward function. This function focuses on evaluating the correctness of the extracted answer from the model's completion.\n",
        "\n",
        "*   It first attempts to extract the answer using the `match_format` regex.\n",
        "*   If no answer is extracted, the model is penalized (-2.0).\n",
        "*   If an answer is extracted, it compares it to the true answer from the dataset.\n",
        "*   An exact match gets the highest reward (5.0).\n",
        "*   A match after stripping whitespace gets a slightly lower reward (3.5).\n",
        "*   If the extracted answer and true answer can be converted to numbers, it calculates a ratio and rewards the model based on how close the ratio is to 1.0.\n",
        "*   Incorrect numerical answers or non-numerical answers that don't match are penalized.\n",
        "\n",
        "This reward function directly incentivizes the model to produce the correct final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e1e99d5"
      },
      "source": [
        "This code block defines a regular expression `match_numbers` specifically to extract potential numerical answers from the model's output, even if they are not perfectly enclosed within the `solution_start` and `solution_end` tokens or include commas. It looks for a number (potentially negative, with decimals and commas) after the `solution_start` token. The `print` statements demonstrate how this regex extracts numbers from various sample strings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97cafa2b"
      },
      "source": [
        "This code block defines the `check_numbers` reward function, which is similar to `check_answer` but specifically focuses on extracting and comparing numerical answers using the `match_numbers` regex.\n",
        "\n",
        "*   It attempts to extract a number using `match_numbers`.\n",
        "*   If no number is extracted, it penalizes the model (-2.5).\n",
        "*   If a number is extracted, it attempts to convert both the extracted guess and the true answer to floating-point numbers, handling commas in the guess.\n",
        "*   It rewards the model (3.5) if the converted numbers are equal and penalizes it (-1.5) otherwise.\n",
        "*   A print statement is included to periodically show the question, true answer, model response, and extracted answer during training, which helps in monitoring the training process.\n",
        "\n",
        "This reward function provides a separate signal to the model based on the numerical correctness of its output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94188c7"
      },
      "source": [
        "This code block calculates the token length of each prompt in the dataset after applying the chat template and adding the generation prompt. It then determines the 90th percentile of these lengths. This `maximum_length` is used to filter out the longest 10% of prompts to prevent excessive sequence lengths during training, which can lead to memory issues or truncation. The dataset is then filtered to include only prompts shorter than or equal to this maximum length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e085b320"
      },
      "source": [
        "This code block sets up the configurations for the main GRPO training.\n",
        "\n",
        "*   `max_prompt_length`: Set based on the calculated maximum length from the dataset analysis.\n",
        "*   `max_completion_length`: Calculated as the remaining length after the prompt, up to the `max_seq_length`.\n",
        "*   `vllm_sampling_params`: Defines the parameters for sampling completions from the model using vLLM during the GRPO process (e.g., temperature, top_p, stop tokens).\n",
        "*   `training_args`: A `GRPOConfig` object containing various training parameters specific to GRPO, such as learning rate, weight decay, optimizer, batch size, number of generations per prompt, and the maximum number of training steps. It also includes optional parameters for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c04625e"
      },
      "source": [
        "This code block initializes and starts the GRPO training process using the `GRPOTrainer`.\n",
        "\n",
        "*   `model`, `processing_class`: The LoRA-enabled model and its tokenizer.\n",
        "*   `reward_funcs`: A list of the reward functions defined earlier (`match_format_exactly`, `match_format_approximately`, `check_answer`, `check_numbers`). The GRPO trainer will use these functions to calculate rewards for the model's generated completions.\n",
        "*   `args`: The `GRPOConfig` containing the training parameters.\n",
        "*   `train_dataset`: The dataset prepared for GRPO training.\n",
        "\n",
        "The `trainer.train()` method starts the GRPO algorithm, which iteratively generates completions, calculates rewards based on the provided functions, and updates the model's parameters to maximize the expected reward. The markdown cell above this provides an example of the training log, showing how the reward should ideally increase over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e8d297"
      },
      "source": [
        "This code block demonstrates how to perform inference with the base model *before* applying the GRPO training. It takes a simple question (\"What is the sqrt of 101?\") and uses `model.fast_generate` with specified `SamplingParams` to get the model's response. This serves as a baseline to compare against the model's performance after GRPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aaf12e7"
      },
      "source": [
        "This code block saves the trained LoRA adapters to a local directory named \"grpo_saved_lora\" using `model.save_lora`. This saves only the small LoRA weights, not the entire base model, making it efficient for storing and sharing the fine-tuned changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "445ed966"
      },
      "source": [
        "This code block verifies that the LoRA adapters were saved correctly and that they contain non-zero values (meaning the training actually updated the weights). It uses `safetensors` to open the saved adapter file and checks that the number of zero elements in the weight tensors is not equal to the total number of elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59bc7f7f"
      },
      "source": [
        "This code block demonstrates how to perform inference using the model with the *trained* LoRA adapters loaded.\n",
        "\n",
        "*   It constructs the input prompt using the chat template, including the system prompt and the user question.\n",
        "*   It uses `model.fast_generate` for inference.\n",
        "*   Crucially, it passes `model.load_lora(\"grpo_saved_lora\")` to the `lora_request` parameter. This tells vLLM to apply the saved LoRA weights during generation, allowing you to see the effect of the GRPO training.\n",
        "\n",
        "The output of this cell should show the model attempting to use the GRPO format and providing a more reasoned answer compared to the inference before training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92714034"
      },
      "source": [
        "This markdown cell provides commentary on the inference results, noting that the model's reasoning has improved after training, even if not always perfectly correct, due to the limited training time in this example. It suggests that extending the training time and sequence length would likely lead to better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ce3016a"
      },
      "source": [
        "This markdown cell introduces the concept of saving the model in different formats for deployment and sharing, specifically focusing on saving to `float16` or `int4` formats and pushing to the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56119013"
      },
      "source": [
        "This code block provides commented-out examples of how to save the fine-tuned model in different merged formats (`merged_16bit` and `merged_4bit`) and how to push these merged models to the Hugging Face Hub. It also shows how to save and push just the LoRA adapters as a fallback. These options are useful for deploying the model for inference with libraries that support merged models or LoRA adapters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c97de6"
      },
      "source": [
        "This markdown cell explains how to save the model in the GGUF format, which is compatible with `llama.cpp` and enables running the model on various hardware platforms, including CPUs. It mentions that Unsloth natively supports GGUF conversion and lists some common quantization methods like `q8_0`, `q4_k_m`, and `q5_k_m`, recommending the K_M methods for better trade-offs between size and performance. It also links to an Ollama notebook for finetuning and auto-exporting to Ollama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "885c541d"
      },
      "source": [
        "This code block provides commented-out examples of how to save the fine-tuned model in different GGUF quantization methods (`q8_0`, `f16`, `q4_k_m`) locally and how to push these GGUF files to the Hugging Face Hub. It also shows how to save and push multiple GGUF quantization methods at once for efficiency. You would uncomment and modify these lines based on your desired GGUF format and Hugging Face repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47c55dd"
      },
      "source": [
        "This final markdown cell concludes the notebook, providing instructions on how to use the saved GGUF file with `llama.cpp`. It also includes links to the Unsloth Discord server for support and community interaction, as well as links to other relevant Unsloth notebooks and documentation."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}